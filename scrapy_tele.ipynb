{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7aa5f4b-b853-43ee-86fa-ef53d0a283fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title **1. [ Required ] Set up your credentials once** { display-mode: \"form\" }\n",
    "\n",
    "# @markdown Here, you need to input your credentials: `username`, `phone`, `api_id`, and `api_hash`. Your `api_id` and `api_hash` can only be generated from [Telegram's app creation page](https://my.telegram.org/apps). Once your credentials are set up, you won’t need to update them again. Just click “Run” to proceed.\n",
    "\n",
    "# Install the Telethon library for Telegram API interactions\n",
    "#!pip install -q telethon\n",
    "\n",
    "# Initial imports\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Telegram imports\n",
    "from telethon.sync import TelegramClient\n",
    "\n",
    "# Google Colab imports\n",
    "#from google.colab import files\n",
    "\n",
    "# Setup / change only the first time you use it\n",
    "# @markdown **1.1.** Your Telegram account username (just 'abc123', not '@'):\n",
    "username = 'username' # @param {type:\"string\"}\n",
    "# @markdown **1.2.** Your Telegram account phone number (ex: '+5511999999999'):\n",
    "phone = '+phone' # @param {type:\"string\"}\n",
    "# @markdown **1.3.** Your API ID, it can be only generated from https://my.telegram.org/apps:\n",
    "api_id = 'number' # @param {type:\"string\"}\n",
    "# @markdown **1.4.** Your API hash, also from https://my.telegram.org/apps:\n",
    "api_hash = 'hasg' # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc7369c-a5d5-4e25-93cd-1e53133afe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file telescrap already exists.\n"
     ]
    }
   ],
   "source": [
    "# @title **2. [ Required ] Adjust every time you want to use it** { display-mode: \"form\" }\n",
    "\n",
    "# @markdown In this section, you will define the parameters for scraping data from Telegram channels or groups. Specify the channels you want to scrape using the format `@ChannelName` or the full URL `https://t.me/ChannelName`. Do not use URLs starting with `https://web.telegram.org/`. Set the date range by defining the start and end day, month, and year. Choose an output file name for the scraped data. Optionally, set a search keyword if you need to filter messages by specific terms. Define the maximum number of messages to scrape and set a timeout in seconds.\n",
    "\n",
    "# Setup / change every time to define scraping parameters\n",
    "\n",
    "# @markdown **2.1.** Here you put the name of the channel or group that you want to scrape, as an example, play: '@LulanoTelegram' or 'https://t.me/LulanoTelegram'. Do not use: 'https://web.telegram.org/a/#-1001249230829' or '-1001249230829'. **Just write the `channel names` always separated by commas (,):**\n",
    "#channels = []\n",
    "respath = 'C:\\\\Users\\\\barto\\\\telescrap\\\\' #homedir\n",
    "%mkdir telescrap\n",
    "channels = ['@tiesiogiaiisukrainos',  '@varlinas', '@volna_lt', '@novayaklaipeda', '@euromore', '@visaginasnews', \n",
    "           '@karas_ukrainoje', '@n_aujenoschat', '@sputniknews_lt', '@lithuanian', '@lithuanianlegio', '@lithuanianews24', \n",
    "           '@vilnius_lithuania', \"@aktyvusklubas2\", '@rudelfi', '@slava_ukraini_ltu', '@infalt', '@NorthernFront_NATO',\n",
    "           '@matricalietuvoje', '@vardantosLietuvos', '@novayaklaipeda', '@litovecrubitpravdu', '@politikai', '@sapereaudelt',\n",
    "           '@atsibudimas', '@Kritinismastytojas', '@n_aujienos', '@neadeqatus', '@hmelisozreli', '@klaipedaonline', \n",
    "           '@Jaunieji_Partizanai', '@naujoji_pasaulio_tvarka', '@karas_Z', '@seimusajudistesiasi','@seimusajudis2021', \n",
    "           '@komentarastv', '@ArunasGl', '@mkzmedia', '@ValomDangu', '@fak_tai', '@mariusjonaitis', '@Karamzin_branch',\n",
    "           '@Karas_ukraina_chronologijos', '@karas_ukrainoje_chronologija', '@RFULietuviskai', '@slava_ukraini_ltu', \n",
    "           '\"@Izraeliokaras', '@zingeris', '@neadeqatus', '@ZoroKanalas', '@ekspertaiTelegram', '@lrtlt', '@radiorlt', \n",
    "           '@baltnews', '@delfi_lietuva', '@zhiznvlitve', '@nexta_live',  '@SolovievLive',  '@rubaltic', '@lietuvasu']\n",
    "\n",
    "\n",
    "#channels = [channel.strip() for channel in channels.split(\",\")]\n",
    "\n",
    "# @markdown **2.2.** Here you can select the `time window` you would like to extract data from the listed communities:\n",
    "date_min = '2023-01-01' # @param {type:\"date\"}\n",
    "date_max = '2025-01-01' # @param {type:\"date\"}\n",
    "\n",
    "date_min = datetime.fromisoformat(date_min).replace(tzinfo=timezone.utc)\n",
    "date_max = datetime.fromisoformat(date_max).replace(tzinfo=timezone.utc)\n",
    "\n",
    "# @markdown **2.3.** Choose a `name` for the final file you want to download as output:\n",
    "file_name = 'Test' # @param {type:\"string\"}\n",
    "\n",
    "# @markdown **2.4.** `Keyword` to search, **leave empty if you want to extract all messages from the channel(s):**\n",
    "key_search = '' # @param {type:\"string\"}\n",
    "\n",
    "# @markdown **2.5.** **Maximum** `number of messages` to scrape (only use if you want a specific limit, otherwise leave a high number to scrape everything):\n",
    "max_t_index = 1000000   # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown **2.6.** `Timeout in seconds` (never leave it longer than 6 hours, that is 21600 seconds, as Google Colab deactivates itself after that time):\n",
    "time_limit = 216000 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown **2.7.** Choose the format of the final file you want to download. If you are a first-time user, choose `Excel`. If you have advanced skills, you can use `Parquet`:\n",
    "File = 'excel' # @param [\"excel\", \"parquet\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df83f41b-0399-4bdc-84a4-9b22676ad270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove invalid XML characters from text\n",
    "def remove_unsupported_characters(text):\n",
    "    valid_xml_chars = (\n",
    "        \"[^\\u0009\\u000A\\u000D\\u0020-\\uD7FF\\uE000-\\uFFFD\"\n",
    "        \"\\U00010000-\\U0010FFFF]\"\n",
    "    )\n",
    "    cleaned_text = re.sub(valid_xml_chars, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to format time in days, hours, minutes, and seconds\n",
    "def format_time(seconds):\n",
    "    days = seconds // 86400\n",
    "    hours = (seconds % 86400) // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f'{int(days):02}:{int(hours):02}:{int(minutes):02}:{int(seconds):02}'\n",
    "\n",
    "# Function to print progress of the scraping process\n",
    "def print_progress(t_index, message_id, start_time, max_t_index):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    current_progress = t_index / (t_index + message_id) if (t_index + message_id) <= max_t_index else t_index / max_t_index\n",
    "    percentage = current_progress * 100\n",
    "    estimated_total_time = elapsed_time / current_progress\n",
    "    remaining_time = estimated_total_time - elapsed_time\n",
    "\n",
    "    elapsed_time_str = format_time(elapsed_time)\n",
    "    remaining_time_str = format_time(remaining_time)\n",
    "\n",
    "    print(f'Progress: {percentage:.2f}% | Elapsed Time: {elapsed_time_str} | Remaining Time: {remaining_time_str}')\n",
    "\n",
    "# Normalize File variable to avoid issues\n",
    "File = re.sub(r'[^a-z]', '', File.lower())  # Converts to lowercase and removes non-alphabetic characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d3d9c3-de36-4093-86bc-dfec63dda3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reaction_info(peer_dict):\n",
    "    if peer_dict['_'] == 'PeerChannel':\n",
    "        type = peer_dict['_']\n",
    "        id =  peer_dict['channel_id']\n",
    "    elif peer_dict['_'] == 'PeerUser':\n",
    "        type = peer_dict['_']\n",
    "        id =  peer_dict['user_id']\n",
    "    elif peer_dict['_'] == 'PeerChat':\n",
    "        type = peer_dict['_']\n",
    "        id =  peer_dict['chat_id']\n",
    "    return type,id\n",
    "\n",
    "\n",
    "def extract_reactions(message):\n",
    "    emoji_string = ''\n",
    "    reaction_ids = ''\n",
    "    reaction_peer_type = ''\n",
    "    try:\n",
    "        if message.reactions:\n",
    "            #check if recent reactions are avalaible:\n",
    "            if message.reactions.recent_reactions:\n",
    "                for reaction_count in message.reactions.recent_reactions:\n",
    "                    emoji = reaction_count.reaction.emoticon\n",
    "                    emoji_string += emoji + \" \" + \"1\" + \" \"\n",
    "                    type, id = extract_reaction_info(reaction_count.peer_id.to_dict())\n",
    "                    reaction_ids += str(id) + \" \" \n",
    "                    reaction_peer_type += type + \" \" \n",
    "            else:\n",
    "                for reaction_count in message.reactions.results:\n",
    "                    emoji = reaction_count.reaction.emoticon\n",
    "                    count = str(reaction_count.count)\n",
    "                    emoji_string += emoji + \" \" + count + \" \"\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print(f'Error processing reactions: {e}')\n",
    "    return emoji_string, reaction_ids, reaction_peer_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a8f40d-ef61-4552-8edc-e5f04e53e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_data_from_message(message, channel, respond_to_id):\n",
    "    #change data format\n",
    "    date_time = message.date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    #clean text\n",
    "    if message.text != None:\n",
    "        cleaned_content = remove_unsupported_characters(message.text)\n",
    "    else:\n",
    "        cleaned_content= \"\"\n",
    "    #check if media\n",
    "    media = 'True' if message.media else 'False'\n",
    "    #working on reactions to the post\n",
    "    emoji_string, reaction_ids, reaction_peer_type = extract_reactions(message)\n",
    "    #extract info about post author\n",
    "    if message.from_id != None:\n",
    "        author_type, author_id = extract_reaction_info(message.from_id.to_dict())\n",
    "    else:\n",
    "        author_type, author_id = extract_reaction_info(message.peer_id.to_dict())\n",
    "    #extract info about forwarded messages\n",
    "    try:\n",
    "        fwd_type, fwd_author_id = extract_reaction_info(message.fwd_from.from_id.to_dict())\n",
    "        fwd_id = message.fwd_from.channel_post if message.fwd_from.channel_post != None else ''\n",
    "        fwd_date  = message.fwd_from.date.strftime('%Y-%m-%d %H:%M:%S') if message.fwd_from.date != None else ''    \n",
    "    except Exception as e:\n",
    "        #print(f'Error processing message: {e}')\n",
    "        fwd_type, fwd_id, fwd_author_id, fwd_date = \"\", \"\", \"\", \"\"            \n",
    "    #extract info about the message responding to    \n",
    "    if message.reply_to != None: \n",
    "        reply_id = int(message.reply_to.reply_to_top_id or 2147483646)\n",
    "        reply_id = min(message.reply_to.reply_to_msg_id, reply_id)\n",
    "        reply_id = int(respond_to_id or reply_id)\n",
    "        prev_message = await client.get_messages(channel, ids = reply_id)\n",
    "        if prev_message != None:\n",
    "            prev_message = prev_message[0] if isinstance(prev_message, list) else prev_message \n",
    "            prev_message_date  = prev_message.date.strftime('%Y-%m-%d %H:%M:%S') if prev_message.date != None else ''\n",
    "            if prev_message.from_id != None:\n",
    "                prev_message_type, prev_message_author_id = extract_reaction_info(prev_message.from_id.to_dict())\n",
    "            elif prev_message.peer_id != None:\n",
    "                prev_message_type, prev_message_author_id = extract_reaction_info(prev_message.peer_id.to_dict()) \n",
    "            else:\n",
    "                prev_message_type, prev_message_author_id = \"\", \"\"\n",
    "        else:\n",
    "            prev_message_type, prev_message_author_id, prev_message_date = \"\", \"\", \"\"\n",
    "    else:\n",
    "        reply_id, prev_message_type, prev_message_author_id, prev_message_date = \"\", \"\", \"\", \"\"\n",
    "    return {\n",
    "        'Message ID': message.id,\n",
    "        'Author ID': author_id,\n",
    "        'Author type': author_type,\n",
    "        'Author': message.post_author,\n",
    "        'Date': date_time,\n",
    "        'Channel': channel,\n",
    "        'Reply to ID': reply_id,\n",
    "        'Reply to author type': prev_message_type,\n",
    "        'Reply to author ID': prev_message_author_id,\n",
    "        'Reply to date': prev_message_date,\n",
    "        'Forwarded from post ID': fwd_id,\n",
    "        'Forwarded from post date': fwd_date,\n",
    "        'Forwarded from author type': fwd_type,\n",
    "        'Forwarded from author ID': fwd_author_id,\n",
    "        'Type': \"Message\",\n",
    "        'Content': cleaned_content,\n",
    "        'Views': message.views,\n",
    "        'Reactions': emoji_string,\n",
    "        'Reactions Peer': reaction_peer_type,\n",
    "        'Reactions IDs': reaction_ids,\n",
    "        'Shares': message.forwards,\n",
    "        'Media': media,\n",
    "        'Url': f'https://t.me/{channel}/{message.id}'.replace('@', '')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1bbe597-9232-4ad8-bdac-81bc3dfa230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title **3. [ Required ] Start Telegram scraping** { display-mode: \"form\" }\n",
    "\n",
    "# @markdown **Attention:** During this step, Telegram may request a verification code. Please monitor your Telegram app and input the required information promptly. Rest assured, all data entered remains secure.\n",
    "\n",
    "t_index = 0  # Tracker for the number of messages processed\n",
    "start_time = time.time()  # Record the start time for the scraping session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8cd46fb-554e-439d-ad6f-4ad95c176251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### @rubaltic scrapping begin,  00000 posts already scraped #####\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 3.51% | Elapsed Time: 00:00:20:40 | Remaining Time: 00:09:28:38\n",
      "From @rubaltic: 01000 contents of 28509\n",
      "Id: 27509 / Date: 2024-10-17 12:03:22\n",
      "Total: 01000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 7.02% | Elapsed Time: 00:00:42:20 | Remaining Time: 00:09:21:05\n",
      "From @rubaltic: 02000 contents of 28506\n",
      "Id: 26506 / Date: 2024-07-24 08:45:22\n",
      "Total: 02000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 10.52% | Elapsed Time: 00:01:01:34 | Remaining Time: 00:08:43:30\n",
      "From @rubaltic: 03000 contents of 28504\n",
      "Id: 25504 / Date: 2024-05-15 16:28:09\n",
      "Total: 03000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 14.03% | Elapsed Time: 00:01:26:20 | Remaining Time: 00:08:48:57\n",
      "From @rubaltic: 04000 contents of 28504\n",
      "Id: 24504 / Date: 2024-03-17 19:34:24\n",
      "Total: 04000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 17.54% | Elapsed Time: 00:01:42:34 | Remaining Time: 00:08:02:10\n",
      "From @rubaltic: 05000 contents of 28502\n",
      "Id: 23502 / Date: 2024-01-24 15:35:06\n",
      "Total: 05000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 21.05% | Elapsed Time: 00:02:00:12 | Remaining Time: 00:07:30:49\n",
      "From @rubaltic: 06000 contents of 28501\n",
      "Id: 22501 / Date: 2023-11-21 15:34:08\n",
      "Total: 06000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 24.56% | Elapsed Time: 00:02:21:58 | Remaining Time: 00:07:16:03\n",
      "From @rubaltic: 07000 contents of 28499\n",
      "Id: 21499 / Date: 2023-10-03 17:59:52\n",
      "Total: 07000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 28.08% | Elapsed Time: 00:02:44:37 | Remaining Time: 00:07:01:44\n",
      "From @rubaltic: 08000 contents of 28494\n",
      "Id: 20494 / Date: 2023-08-08 09:57:12\n",
      "Total: 08000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 31.59% | Elapsed Time: 00:03:06:33 | Remaining Time: 00:06:44:04\n",
      "From @rubaltic: 09000 contents of 28493\n",
      "Id: 19493 / Date: 2023-06-13 07:27:47\n",
      "Total: 09000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 35.11% | Elapsed Time: 00:03:29:10 | Remaining Time: 00:06:26:32\n",
      "From @rubaltic: 10000 contents of 28479\n",
      "Id: 18479 / Date: 2023-04-19 13:15:13\n",
      "Total: 10000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 38.63% | Elapsed Time: 00:03:47:23 | Remaining Time: 00:06:01:12\n",
      "From @rubaltic: 11000 contents of 28474\n",
      "Id: 17474 / Date: 2023-03-06 15:02:14\n",
      "Total: 11000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 42.15% | Elapsed Time: 00:04:08:34 | Remaining Time: 00:05:41:10\n",
      "From @rubaltic: 12000 contents of 28470\n",
      "Id: 16470 / Date: 2023-01-20 07:33:11\n",
      "Total: 12000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### @rubaltic was ok with 12345 posts #####\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### @lietuvasu scrapping begin,  00000 posts already scraped #####\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n",
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Progress: 46.91% | Elapsed Time: 00:04:30:29 | Remaining Time: 00:05:06:05\n",
      "From @lietuvasu: 00655 contents of 15366\n",
      "Id: 14711 / Date: 2024-11-19 09:19:23\n",
      "Total: 13000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n",
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n",
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n",
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n",
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n",
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Progress: 50.60% | Elapsed Time: 00:04:58:42 | Remaining Time: 00:04:51:39\n",
      "From @lietuvasu: 01655 contents of 15325\n",
      "Id: 13670 / Date: 2024-08-23 14:48:04\n",
      "Total: 14000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n",
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Progress: 54.30% | Elapsed Time: 00:05:19:53 | Remaining Time: 00:04:29:15\n",
      "From @lietuvasu: 02655 contents of 15281\n",
      "Id: 12626 / Date: 2024-05-27 12:19:57\n",
      "Total: 15000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n",
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n",
      "Server closed the connection: [WinError 1236] The network connection was aborted by the local system\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Progress: 58.02% | Elapsed Time: 00:05:51:45 | Remaining Time: 00:04:14:33\n",
      "From @lietuvasu: 03655 contents of 15234\n",
      "Id: 11579 / Date: 2024-03-17 12:46:49\n",
      "Total: 16000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 61.71% | Elapsed Time: 00:06:10:26 | Remaining Time: 00:03:49:53\n",
      "From @lietuvasu: 04655 contents of 15205\n",
      "Id: 10550 / Date: 2024-02-02 19:50:12\n",
      "Total: 17000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 65.48% | Elapsed Time: 00:06:30:06 | Remaining Time: 00:03:25:41\n",
      "From @lietuvasu: 05655 contents of 15146\n",
      "Id: 09491 / Date: 2023-12-22 18:29:51\n",
      "Total: 18000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 69.21% | Elapsed Time: 00:06:51:52 | Remaining Time: 00:03:03:11\n",
      "From @lietuvasu: 06655 contents of 15106\n",
      "Id: 08451 / Date: 2023-10-14 18:06:44\n",
      "Total: 19000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 72.93% | Elapsed Time: 00:07:14:17 | Remaining Time: 00:02:41:13\n",
      "From @lietuvasu: 07655 contents of 15080\n",
      "Id: 07425 / Date: 2023-08-03 12:54:14\n",
      "Total: 20000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 76.67% | Elapsed Time: 00:07:32:47 | Remaining Time: 00:02:17:45\n",
      "From @lietuvasu: 08655 contents of 15044\n",
      "Id: 06389 / Date: 2023-05-21 09:46:37\n",
      "Total: 21000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 80.49% | Elapsed Time: 00:07:53:41 | Remaining Time: 00:01:54:49\n",
      "From @lietuvasu: 09655 contents of 14988\n",
      "Id: 05333 / Date: 2023-03-23 09:45:03\n",
      "Total: 22000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Progress: 84.36% | Elapsed Time: 00:08:08:45 | Remaining Time: 00:01:30:36\n",
      "From @lietuvasu: 10655 contents of 14919\n",
      "Id: 04264 / Date: 2023-01-22 20:18:11\n",
      "Total: 23000 contents until now\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##### @lietuvasu was ok with 11003 posts #####\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scraping process\n",
    "for channel in channels:\n",
    "    if t_index >= max_t_index:\n",
    "        break\n",
    "\n",
    "    if time.time() - start_time > time_limit:\n",
    "        break\n",
    "\n",
    "    loop_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        c_index = 0\n",
    "        used_ids = set()\n",
    "        data = set() # List to store scraped data\n",
    "        print(f'\\n\\n##### {channel} scrapping begin,  {len(used_ids):05} posts already scraped #####\\n\\n')\n",
    "        async with TelegramClient(username, api_id, api_hash) as client:\n",
    "            async for message in client.iter_messages(channel, search=key_search):\n",
    "                if message.id in used_ids:\n",
    "                    continue \n",
    "                try:\n",
    "                    if date_min <= message.date <= date_max:\n",
    "                        msg = await extract_data_from_message(message, channel, None)\n",
    "                        used_ids.add(message.id)\n",
    "                        data.add(tuple(msg.items()))\n",
    "                        #comment/replies processing\n",
    "                        if message.replies != None and message.replies.replies > 0:\n",
    "                            async for comment_message in client.iter_messages(channel, reply_to=message.id):\n",
    "                                try:\n",
    "                                    if comment_message.id in used_ids:\n",
    "                                        continue \n",
    "                                    msg = await extract_data_from_message(comment_message, channel, message.id)\n",
    "                                    used_ids.add(comment_message.id)\n",
    "                                    data.add(tuple(msg.items()))\n",
    "                                except Exception as e:\n",
    "                                    print(f'Error processing comment: {e}')\n",
    "                        c_index += 1\n",
    "                        t_index += 1\n",
    "\n",
    "                        # Print progress\n",
    "                        if t_index % 1000 == 0:\n",
    "                            print(f'{\"-\" * 80}')\n",
    "                            print_progress(t_index, message.id, start_time, max_t_index)\n",
    "                            current_max_id = min(c_index + message.id, max_t_index)\n",
    "                            print(f'From {channel}: {c_index:05} contents of {current_max_id:05}')\n",
    "                            print(f'Id: {message.id:05} / Date: {msg['Date']}')\n",
    "                            print(f'Total: {t_index:05} contents until now')\n",
    "                            print(f'{\"-\" * 80}\\n\\n')\n",
    "\n",
    "                        if t_index % 10000 == 0:\n",
    "                            if File == 'parquet':\n",
    "                                backup_filename = respath + f'backup_{file_name}_until_{t_index:05}_{channel}_ID{message.id:07}.parquet'\n",
    "                                pd.DataFrame([dict(m) for m in data]).to_parquet(backup_filename, index=False)\n",
    "                            elif File == 'excel':\n",
    "                                backup_filename = respath  + f'backup_{file_name}_until_{t_index:05}_{channel}_ID{message.id:07}.xlsx'\n",
    "                                pd.DataFrame([dict(m) for m in data]).to_excel(backup_filename, index=False, engine='openpyxl')\n",
    "\n",
    "                        if t_index >= max_t_index:\n",
    "                            break\n",
    "                        if time.time() - start_time > time_limit:\n",
    "                            break\n",
    "                    elif message.date < date_min:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f'Error processing message: {e}')\n",
    "            print(f'\\n\\n##### {channel} was ok with {c_index:05} posts #####\\n\\n')\n",
    "            dt = [dict(m) for m in data]\n",
    "            df = pd.DataFrame(dt)\n",
    "            if File == 'parquet':\n",
    "                partial_filename = respath + f'complete_{channel}_in_{file_name}_until_{t_index:05}.parquet'\n",
    "                df.to_parquet(partial_filename, index=False)\n",
    "            elif File == 'excel':\n",
    "                partial_filename = respath + f'complete_{channel}_in_{file_name}_until_{t_index:05}.xlsx'\n",
    "                df.to_excel(partial_filename, index=False, engine='openpyxl')\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'{channel} error: {e}')\n",
    "\n",
    "    loop_end_time = time.time()\n",
    "    loop_duration = loop_end_time - loop_start_time\n",
    "\n",
    "    if loop_duration < 60:\n",
    "        time.sleep(60 - loop_duration)\n",
    "\n",
    "#print(f'\\n{\"-\" * 50}\\n#Concluded! #{t_index:05} posts were scraped!\\n{\"-\" * 50}\\n\\n\\n\\n')\n",
    "#dt = [dict(m) for m in data]\n",
    "#df = pd.DataFrame(dt)\n",
    "#if File == 'parquet':\n",
    "#    final_filename = respath + f'FINAL_{file_name}_with_{t_index:05}.parquet'\n",
    "#    df.to_parquet(final_filename, index=False)\n",
    "#elif File == 'excel':\n",
    "#    final_filename = respath + f'FINAL_{file_name}_with_{t_index:05}.xlsx'\n",
    "#    df.to_excel(final_filename, index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8aef09-217f-4373-973a-28defd8eef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
